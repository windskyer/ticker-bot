import time
import random
from datetime import datetime
import google.generativeai as genai
from config import cfg

API_KEY = cfg["gemini"]["key"]

# === Gemini API Key é…ç½® ===
genai.configure(api_key=API_KEY)


MODEL_CANDIDATES = [
    "models/gemini-2.5-pro",   # ä½ çŽ°åœ¨ç”¨çš„
    "models/gemini-1.5-pro",   # ç¨³å®š/é•¿ä¸Šä¸‹æ–‡
    "models/gemini-1.5-flash"  # æ›´å¿«/æ›´ä¾¿å®œï¼Œå¯åšå…œåº•
]


def safe_extract(response):
    """ä»Ž Gemini response é‡Œå®‰å…¨æå–æ–‡æœ¬"""
    if not hasattr(response, "candidates") or not response.candidates:
        return None
    texts = []
    for c in response.candidates:
        if getattr(c, "content", None) and c.content.parts:
            for p in c.content.parts:
                t = getattr(p, "text", None)
                if t and t.strip():
                    texts.append(t.strip())
    return "\n".join(texts) if texts else None


def call_gemini_with_retry(
    prompt: str,
    hard_timeout_s: int = 300,         # æ•´ä½“æœ€é•¿ç­‰å¾…æ—¶é—´ï¼ˆå»ºè®® 2~5 åˆ†é’Ÿï¼‰
    per_call_timeout_s: int = 30,      # å•æ¬¡è¯·æ±‚è¶…æ—¶
    max_backoff_s: int = 64,           # æœ€å¤§é€€é¿æ—¶é—´
    temperature: float = 0.7,
    max_output_tokens: int = 1024,
    debug: bool = False
) -> str:
    start = time.time()
    attempt = 0
    last_exc = None
    model_idx = 0

    while time.time() - start < hard_timeout_s:
        attempt += 1
        model_name = MODEL_CANDIDATES[model_idx % len(MODEL_CANDIDATES)]
        model_idx += 1
        if debug:
            print(f"ðŸ”„ å°è¯• #{attempt}ï¼Œæ¨¡åž‹ï¼š{model_name}")

        try:
            model = genai.GenerativeModel(
                model_name,
                generation_config={
                    "temperature": temperature,
                    "max_output_tokens": max_output_tokens,
                },
                # å¯æŒ‰éœ€æ”¾å®½å®‰å…¨é˜ˆå€¼ï¼Œå‡å°‘â€œç©ºè¾“å‡ºâ€æ¦‚çŽ‡ï¼ˆè‹¥ä½ åœºæ™¯çº¯é‡‘èžæ–‡æœ¬ï¼‰
                # safety_settings={"HARASSMENT": "BLOCK_NONE", ...}
            )

            resp = model.generate_content(
                prompt,
                request_options={"timeout": per_call_timeout_s}
            )
            text = safe_extract(resp)
            if text:
                return text

            # è‹¥è¢«å®‰å…¨ç­–ç•¥æ‹¦æˆªï¼Œæ‰“å°æç¤ºï¼ˆdebugï¼‰
            if debug:
                try:
                    d = resp.to_dict()
                    pf = d.get("promptFeedback") or {}
                    br = pf.get("blockReason")
                    if br:
                        print(f"ðŸ›¡ï¸ è¢«å®‰å…¨ç­–ç•¥æ‹¦æˆªï¼š{br}")
                except Exception:
                    pass

        except Exception as e:
            last_exc = e
            if debug:
                print(f"âŒ è°ƒç”¨å¼‚å¸¸ï¼š{e}")

        # æŒ‡æ•°é€€é¿ + æŠ–åŠ¨
        sleep_s = min(max_backoff_s, 2 ** min(attempt, 6)) + random.uniform(0, 1.0)
        if debug:
            print(f"â³ ç­‰å¾… {sleep_s:.1f}s åŽé‡è¯•...")
        time.sleep(sleep_s)

    # åˆ°è¾¾ç¡¬è¶…æ—¶è¿˜æ²¡æ‹¿åˆ°å†…å®¹
    msg = "âš ï¸ ç”Ÿæˆå¤±è´¥ï¼šå¤šæ¨¡åž‹å¤šæ¬¡é‡è¯•åŽä»æ— å†…å®¹ã€‚"
    if last_exc and debug:
        msg += f"\næœ€åŽé”™è¯¯ï¼š{last_exc}"
    return msg


def generate_report(stock_data, debug=False):
    today = datetime.now().strftime("%Y-%m-%d")
    # æŽ§åˆ¶è¾“å…¥ä½“ç§¯ï¼Œé¿å…è¿‡é•¿å¯¼è‡´ 500/è¶…æ—¶ï¼›åªä¼ éœ€è¦çš„ä¿¡æ¯
    prompt = f"""
ä»Šå¤©æ˜¯ {today}ã€‚ä»¥ä¸‹æ˜¯æœ€æ–°çš„æ”¶ç›˜ä»·ï¼ˆå·²ç²¾ç®€ï¼‰ï¼š
{stock_data}

è¯·ç”Ÿæˆä¸€ä»½ç®€çŸ­çš„é‡‘èžæ—¥æŠ¥ï¼ŒåŒ…å«ï¼š
1) å¸‚åœºæ•´ä½“è¶‹åŠ¿
2) æ¯åªè‚¡ç¥¨çš„ç®€è¦ç‚¹è¯„ï¼ˆå¯åˆç†æŽ¨æµ‹é©±åŠ¨å› ç´ ï¼‰
3) æœªæ¥é£Žé™©æˆ–æœºä¼šæç¤º
è¦æ±‚ï¼šä¸­æ–‡ã€ä¸“ä¸šã€ç²¾ç‚¼ã€‚è‹¥ä¿¡æ¯ä¸è¶³ï¼Œè¯·è¿”å›žâ€œä»Šæ—¥æš‚æ— æ•°æ®â€ã€‚
"""

    content = call_gemini_with_retry(
        prompt,
        hard_timeout_s=300,      # å¯æŒ‰éœ€è°ƒæ•´æ•´ä½“æœ€é•¿ç­‰å¾…æ—¶é—´
        per_call_timeout_s=30,   # å•æ¬¡è¯·æ±‚è¶…æ—¶
        max_backoff_s=64,
        temperature=0.7,
        max_output_tokens=900,
        debug=debug
    )
    print(f"\n[{datetime.now()}] === é‡‘èžæ—¥æŠ¥å·²ç”Ÿæˆ ===\n")
    return content


# ====== AIæ—¥æŠ¥ ======
def generate_report_macro(data, debug=False):
    today = datetime.now().strftime("%Y-%m-%d")
    latest = data.tail(1).iloc[0].to_dict()

    prompt = f"""
ä»Šå¤©æ˜¯ {today}ã€‚ä»¥ä¸‹æ˜¯æœ€æ–°çš„å®è§‚èµ„äº§æ”¶ç›˜ä»·ï¼š
{latest}

è¯·ç”¨ä¸­æ–‡å†™ä¸€ä»½ç®€çŸ­çš„é‡‘èžæ—¥æŠ¥ï¼Œæ€»ç»“ï¼š
1. å®è§‚å¸‚åœºæ•´ä½“è¶‹åŠ¿
2. å››ç±»èµ„äº§çš„è¡¨çŽ°å’Œç®€è¦ç‚¹è¯„
3. é£Žé™©æˆ–æœºä¼šæç¤º
è¦æ±‚ä¸“ä¸šç®€æ´ã€‚
"""
    content = call_gemini_with_retry(
        prompt,
        hard_timeout_s=300,  # å¯æŒ‰éœ€è°ƒæ•´æ•´ä½“æœ€é•¿ç­‰å¾…æ—¶é—´
        per_call_timeout_s=30,  # å•æ¬¡è¯·æ±‚è¶…æ—¶
        max_backoff_s=64,
        temperature=0.7,
        max_output_tokens=900,
        debug=debug
    )
    print(f"\n[{datetime.now()}] === é‡‘èžæ—¥æŠ¥å·²ç”Ÿæˆ ===\n")
    return content
